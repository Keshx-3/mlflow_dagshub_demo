# MLflow Model Management with DAGsHub

This project explores **MLflowâ€™s experiment tracking and model registry** with an **end-to-end workflow** on **DAGsHub**. It demonstrates how to manage ML experiments, track model performance, and implement a **Champion-Challenger framework**.

## ğŸ“Œ Project Overview
- **Data Generation**: Created an **imbalanced dataset** using `make_classification`.
- **Model Training**: Trained **Logistic Regression, Random Forest, XGBoost**, and **SMOTETomek-balanced XGBoost** models.
- **Experiment Tracking**: Logged **metrics, parameters, and models** in MLflow.
- **DAGsHub Integration**: Set up a **remote MLflow tracking server** on DAGsHub.
- **Model Registry**: Implemented a **Champion-Challenger model framework** for comparing models.

## ğŸ› ï¸ Technologies Used
- **Python**
- **MLflow** for experiment tracking
- **DAGsHub** for centralized MLflow tracking and collaboration
- **scikit-learn, XGBoost** for model training
- **SMOTETomek** for handling class imbalance

## ğŸš€ Why DAGsHub?
ğŸ”¹ **Version Control for Code + Data** ğŸ“  
ğŸ”¹ **Centralized Experiment Tracking with MLflow** ğŸ“Š  
ğŸ”¹ **Collaboration for ML Teams** ğŸ‘¥  

## ğŸ“¸ Experiments
![Anomaly_Detection](https://github.com/user-attachments/assets/fd926e7b-d1a2-4a21-8355-79638f5f3918)

## ğŸ”— Links
- **MLflow Experiment Tracking**: [DAGsHub Link]
- **Project Code**: [GitHub Link]

Would love to get feedback or contributionsâ€”feel free to check it out! ğŸš€
